{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a9b85-d028-469e-86b1-94509d578e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447c154e-6c25-43bf-adc3-a0dfe979abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qiskit in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: qiskit-aer in ./venv/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: imbalanced-learn in ./venv/lib/python3.12/site-packages (0.14.1)\n",
      "Requirement already satisfied: rustworkx>=0.15.0 in ./venv/lib/python3.12/site-packages (from qiskit) (0.17.1)\n",
      "Requirement already satisfied: numpy<3,>=1.17 in ./venv/lib/python3.12/site-packages (from qiskit) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.5 in ./venv/lib/python3.12/site-packages (from qiskit) (1.16.3)\n",
      "Requirement already satisfied: dill>=0.3 in ./venv/lib/python3.12/site-packages (from qiskit) (0.4.0)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in ./venv/lib/python3.12/site-packages (from qiskit) (5.6.0)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.12/site-packages (from qiskit) (4.15.0)\n",
      "Requirement already satisfied: psutil>=5 in ./venv/lib/python3.12/site-packages (from qiskit-aer) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in ./venv/lib/python3.12/site-packages (from qiskit-aer) (2.9.0.post0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in ./venv/lib/python3.12/site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in ./venv/lib/python3.12/site-packages (from imbalanced-learn) (0.1.5)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in ./venv/lib/python3.12/site-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in ./venv/lib/python3.12/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install qiskit qi0skit-aer imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914d23e1-62cc-4891-a2a1-d10353f00442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, confusion_matrix, \n",
    "                             roc_curve, classification_report)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c638410c-5eb0-4196-a31c-200b92565beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Directory structure created\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('output/data', exist_ok=True)\n",
    "os.makedirs('output/models', exist_ok=True)\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "print(\" Directory structure created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef387c7-4f8d-467e-bc44-4f88e2503eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: DATA LOADING & EXPLORATION\n",
      "\n",
      "Raw Dataset Shape: (100000, 8)\n",
      "Columns: ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price', 'repeat_retailer', 'used_chip', 'used_pin_number', 'online_order', 'fraud']\n",
      "\n",
      "Class Distribution:\n",
      "fraud\n",
      "0    91260\n",
      "1     8740\n",
      "Name: count, dtype: int64\n",
      "Fraud Percentage: 8.74%\n",
      "\n",
      "Missing Values:\n",
      "distance_from_home                3\n",
      "distance_from_last_transaction    2\n",
      "ratio_to_median_purchase_price    3\n",
      "repeat_retailer                   3\n",
      "used_chip                         2\n",
      "used_pin_number                   3\n",
      "online_order                      5\n",
      "fraud                             0\n",
      "dtype: int64\n",
      "\n",
      " Data exploration complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 1: DATA LOADING & EXPLORATION\")\n",
    "\n",
    "df_raw = pd.read_csv('dataset.csv')\n",
    "df_raw.columns = df_raw.columns.str.strip().str.lower()\n",
    "\n",
    "print(f\"\\nRaw Dataset Shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df_raw['fraud'].value_counts())\n",
    "print(f\"Fraud Percentage: {df_raw['fraud'].mean() * 100:.2f}%\")\n",
    "print(f\"\\nMissing Values:\\n{df_raw.isnull().sum()}\")\n",
    "\n",
    "# Save data info\n",
    "data_info = {\n",
    "    'raw_shape': df_raw.shape,\n",
    "    'columns': list(df_raw.columns),\n",
    "    'fraud_percentage': float(df_raw['fraud'].mean() * 100),\n",
    "    'missing_values': df_raw.isnull().sum().to_dict(),\n",
    "    'class_distribution': df_raw['fraud'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "with open('output/data/data_info.json', 'w') as f:\n",
    "    json.dump(data_info, f, indent=2)\n",
    "\n",
    "print(\"\\n Data exploration complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5d1ef1-3aa2-4209-82d3-a88a7ae77d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: FEATURE ENGINEERING & SELECTION\n",
      "\n",
      "Removing outliers...\n",
      "Removed 34470 outliers (34.5%)\n",
      "\n",
      "Top Features by Correlation:\n",
      "ratio_to_median_purchase_price    0.587869\n",
      "online_order                      0.156824\n",
      "distance_from_home                0.002360\n",
      "used_chip                         0.001974\n",
      "distance_from_last_transaction    0.000457\n",
      "dtype: float64\n",
      "\n",
      "Performing feature selection...\n",
      "Selected Features: ['distance_from_home', 'ratio_to_median_purchase_price', 'used_chip', 'online_order']\n",
      "\n",
      " Feature engineering complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: FEATURE ENGINEERING & SELECTION\")\n",
    "\n",
    "X = df_raw.drop('fraud', axis=1)\n",
    "y = df_raw['fraud']\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Outlier removal (IQR method)\n",
    "print(\"\\nRemoving outliers...\")\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_mask = ~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)\n",
    "X_clean = X[outlier_mask]\n",
    "y_clean = y[outlier_mask]\n",
    "print(f\"Removed {len(X) - len(X_clean)} outliers ({(1-len(X_clean)/len(X))*100:.1f}%)\")\n",
    "\n",
    "# Remove constant features\n",
    "X_clean = X_clean.loc[:, X_clean.nunique() > 1]\n",
    "\n",
    "# Feature correlation analysis\n",
    "correlations = X_clean.corrwith(y_clean).abs().sort_values(ascending=False)\n",
    "print(f\"\\nTop Features by Correlation:\")\n",
    "print(correlations.head())\n",
    "\n",
    "# Feature Selection using Mutual Information\n",
    "print(\"\\nPerforming feature selection...\")\n",
    "selector = SelectKBest(mutual_info_classif, k=4)\n",
    "X_selected = selector.fit_transform(X_clean, y_clean)\n",
    "selected_features = X_clean.columns[selector.get_support()].tolist()\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "\n",
    "# Save processed data\n",
    "df_processed = pd.DataFrame(X_selected, columns=selected_features)\n",
    "df_processed['Class'] = y_clean.values\n",
    "df_processed.to_csv('output/data/processed_dataset.csv', index=False)\n",
    "\n",
    "feature_info = {\n",
    "    'original_features': list(X.columns),\n",
    "    'selected_features': selected_features,\n",
    "    'selection_method': 'Mutual Information',\n",
    "    'feature_scores': {feat: float(score) for feat, score in \n",
    "                       zip(selected_features, selector.scores_[selector.get_support()])}\n",
    "}\n",
    "\n",
    "with open('output/data/feature_selection.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(\"\\n Feature engineering complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532d8fc1-a4e1-4d15-9e64-b0022e09fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: DATA PARTITIONING & SCALING\n",
      "\n",
      "Data Partition Sizes:\n",
      "  Training:   39311 samples (60.0%)\n",
      "  Validation:  6560 samples (10.0%)\n",
      "  Test:       19659 samples (30.0%)\n",
      "\n",
      "Applying SMOTETomek to training data...\n",
      "Balanced Training: 75146 samples\n",
      "Class Distribution: [37573 37573]\n",
      "\n",
      " Data partitioning complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 3: DATA PARTITIONING & SCALING\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_selected, y_clean, test_size=0.30, random_state=42, stratify=y_clean\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.143, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData Partition Sizes:\")\n",
    "print(f\"  Training:   {len(X_train):5d} samples ({len(X_train)/len(X_selected)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val):5d} samples ({len(X_val)/len(X_selected)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(X_test):5d} samples ({len(X_test)/len(X_selected)*100:.1f}%)\")\n",
    "\n",
    "scaler_classical = RobustScaler()\n",
    "X_train_scaled = scaler_classical.fit_transform(X_train)\n",
    "X_val_scaled = scaler_classical.transform(X_val)\n",
    "X_test_scaled = scaler_classical.transform(X_test)\n",
    "\n",
    "scaler_quantum = MinMaxScaler(feature_range=(0, np.pi))\n",
    "X_train_quantum = scaler_quantum.fit_transform(X_train)\n",
    "X_val_quantum = scaler_quantum.transform(X_val)\n",
    "X_test_quantum = scaler_quantum.transform(X_test)\n",
    "\n",
    "with open('output/models/scaler_classical.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_classical, f)\n",
    "with open('output/models/scaler_quantum.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_quantum, f)\n",
    "\n",
    "print(\"\\nApplying SMOTETomek to training data...\")\n",
    "resampler = SMOTETomek(random_state=42)\n",
    "X_train_balanced, y_train_balanced = resampler.fit_resample(X_train_scaled, y_train)\n",
    "print(f\"Balanced Training: {len(X_train_balanced)} samples\")\n",
    "print(f\"Class Distribution: {np.bincount(y_train_balanced.astype(int))}\")\n",
    "\n",
    "print(\"\\n Data partitioning complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef815f61-647e-44b2-9ea1-22ba31b65829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: TRAINING CLASSICAL BASELINES\n",
      "\n",
      "→ Training Logistic Regression...\n",
      "  Val AUC: 0.9999 | Test AUC: 0.9999\n",
      "\n",
      "→ Training Random Forest...\n",
      "  Val AUC: 1.0000 | Test AUC: 1.0000\n",
      "\n",
      "→ Training Gradient Boosting...\n",
      "  Val AUC: 1.0000 | Test AUC: 1.0000\n",
      "\n",
      "→ Training Neural Network...\n",
      "  Val AUC: 1.0000 | Test AUC: 1.0000\n",
      "\n",
      " Classical baselines complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 4: TRAINING CLASSICAL BASELINES\")\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "models_dict = {}\n",
    "\n",
    "print(\"\\n→ Training Logistic Regression...\")\n",
    "lr = LogisticRegression(C=0.1, max_iter=1000, solver='saga', \n",
    "                       class_weight='balanced', random_state=42)\n",
    "lr.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_val_proba_lr = lr.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_pred_lr = lr.predict(X_test_scaled)\n",
    "y_test_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'Val_AUC': roc_auc_score(y_val, y_val_proba_lr),\n",
    "    'Test_AUC': roc_auc_score(y_test, y_test_proba_lr),\n",
    "    'Test_Accuracy': accuracy_score(y_test, y_test_pred_lr),\n",
    "    'Test_Precision': precision_score(y_test, y_test_pred_lr),\n",
    "    'Test_Recall': recall_score(y_test, y_test_pred_lr),\n",
    "    'Test_F1': f1_score(y_test, y_test_pred_lr)\n",
    "}\n",
    "predictions['lr'] = y_test_proba_lr\n",
    "models_dict['lr'] = lr\n",
    "print(f\"  Val AUC: {results['Logistic Regression']['Val_AUC']:.4f} | Test AUC: {results['Logistic Regression']['Test_AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n→ Training Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "                           class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_val_proba_rf = rf.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_pred_rf = rf.predict(X_test_scaled)\n",
    "y_test_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'Val_AUC': roc_auc_score(y_val, y_val_proba_rf),\n",
    "    'Test_AUC': roc_auc_score(y_test, y_test_proba_rf),\n",
    "    'Test_Accuracy': accuracy_score(y_test, y_test_pred_rf),\n",
    "    'Test_Precision': precision_score(y_test, y_test_pred_rf),\n",
    "    'Test_Recall': recall_score(y_test, y_test_pred_rf),\n",
    "    'Test_F1': f1_score(y_test, y_test_pred_rf)\n",
    "}\n",
    "predictions['rf'] = y_test_proba_rf\n",
    "models_dict['rf'] = rf\n",
    "print(f\"  Val AUC: {results['Random Forest']['Val_AUC']:.4f} | Test AUC: {results['Random Forest']['Test_AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n→ Training Gradient Boosting...\")\n",
    "gb = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, \n",
    "                               max_depth=5, subsample=0.8, random_state=42)\n",
    "gb.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_val_proba_gb = gb.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_pred_gb = gb.predict(X_test_scaled)\n",
    "y_test_proba_gb = gb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "results['Gradient Boosting'] = {\n",
    "    'Val_AUC': roc_auc_score(y_val, y_val_proba_gb),\n",
    "    'Test_AUC': roc_auc_score(y_test, y_test_proba_gb),\n",
    "    'Test_Accuracy': accuracy_score(y_test, y_test_pred_gb),\n",
    "    'Test_Precision': precision_score(y_test, y_test_pred_gb),\n",
    "    'Test_Recall': recall_score(y_test, y_test_pred_gb),\n",
    "    'Test_F1': f1_score(y_test, y_test_pred_gb)\n",
    "}\n",
    "predictions['gb'] = y_test_proba_gb\n",
    "models_dict['gb'] = gb\n",
    "print(f\"  Val AUC: {results['Gradient Boosting']['Val_AUC']:.4f} | Test AUC: {results['Gradient Boosting']['Test_AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n→ Training Neural Network...\")\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 32, 16), activation='relu',\n",
    "                  solver='adam', alpha=0.001, learning_rate='adaptive',\n",
    "                  max_iter=300, random_state=42, early_stopping=True)\n",
    "nn.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_val_proba_nn = nn.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_pred_nn = nn.predict(X_test_scaled)\n",
    "y_test_proba_nn = nn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "results['Neural Network'] = {\n",
    "    'Val_AUC': roc_auc_score(y_val, y_val_proba_nn),\n",
    "    'Test_AUC': roc_auc_score(y_test, y_test_proba_nn),\n",
    "    'Test_Accuracy': accuracy_score(y_test, y_test_pred_nn),\n",
    "    'Test_Precision': precision_score(y_test, y_test_pred_nn),\n",
    "    'Test_Recall': recall_score(y_test, y_test_pred_nn),\n",
    "    'Test_F1': f1_score(y_test, y_test_pred_nn)\n",
    "}\n",
    "predictions['nn'] = y_test_proba_nn\n",
    "models_dict['nn'] = nn\n",
    "print(f\"  Val AUC: {results['Neural Network']['Val_AUC']:.4f} | Test AUC: {results['Neural Network']['Test_AUC']:.4f}\")\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    with open(f'output/models/{name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "print(\"\\n Classical baselines complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a475ebc-446b-49a8-bd0b-75a97479f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: QUANTUM MODEL SETUP\n",
      "\n",
      "Quantum Configuration:\n",
      "  Qubits: 4\n",
      "  Backend: AerSimulator\n",
      "\n",
      " Quantum circuit defined with 3 layers and Parity Measurement\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 5: QUANTUM MODEL SETUP\")\n",
    "\n",
    "n_qubits = 4  \n",
    "simulator = AerSimulator()\n",
    "NUM_LAYERS = 3 \n",
    "\n",
    "print(f\"\\nQuantum Configuration:\")\n",
    "print(f\"  Qubits: {n_qubits}\")\n",
    "print(f\"  Backend: AerSimulator\")\n",
    "\n",
    "def feature_map(x):\n",
    "    \"\"\"Encode classical data using Angle Encoding\"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    for i in range(n_qubits):\n",
    "        qc.ry(x[i], i)\n",
    "    return qc\n",
    "\n",
    "def variational_layer(params):\n",
    "    \"\"\"Trainable layers with Circular Entanglement\"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    p = params.reshape(NUM_LAYERS, n_qubits)\n",
    "    \n",
    "    for l in range(NUM_LAYERS):\n",
    "        for i in range(n_qubits):\n",
    "            qc.ry(p[l, i], i)\n",
    "        \n",
    "        for i in range(n_qubits):\n",
    "            qc.cx(i, (i + 1) % n_qubits)\n",
    "            \n",
    "    for i in range(n_qubits):\n",
    "        qc.ry(p[0, i], i)\n",
    "        \n",
    "    return qc\n",
    "\n",
    "def quantum_model(x, params):\n",
    "    \"\"\"Complete circuit composition\"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    qc.compose(feature_map(x), inplace=True)\n",
    "    qc.compose(variational_layer(params), inplace=True)\n",
    "    return qc\n",
    "\n",
    "def quantum_forward(x, params, shots=2048):\n",
    "    \"\"\"Forward pass using Parity measurement\"\"\"\n",
    "    qc = quantum_model(x, params)\n",
    "    qc.measure_all()\n",
    "    result = simulator.run(qc, shots=shots).result()\n",
    "    counts = result.get_counts()\n",
    "    \n",
    "    exp = 0\n",
    "    for bitstring, count in counts.items():\n",
    "        # Parity logic: captures interactions across ALL qubits\n",
    "        parity = (-1)**bitstring.count('1')\n",
    "        exp += parity * count\n",
    "    return exp / shots\n",
    "\n",
    "def quantum_predict_proba(x, params, shots=2048):\n",
    "    \"\"\"Convert expectation [-1, 1] to probability [0, 1]\"\"\"\n",
    "    exp = quantum_forward(x, params, shots)\n",
    "    return (exp + 1) / 2\n",
    "\n",
    "print(\"\\n Quantum circuit defined with 3 layers and Parity Measurement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0d7ab3-777a-4067-815e-0f8c2a8e7651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: QUANTUM TRAINING\n",
      "\n",
      "Training Configuration:\n",
      "  Training samples: 2000\n",
      "  Fraud: 1000 | Non-fraud: 1000\n",
      "\n",
      "Starting training with COBYLA optimizer...\n",
      "Epoch  1/30 | Optimizing... | Time: 15.9s\n",
      "Epoch  2/30 | Optimizing... | Time: 26.0s\n",
      "Epoch  3/30 | Optimizing... | Time: 18.0s\n",
      "Epoch  4/30 | Optimizing... | Time: 19.2s\n",
      "Epoch  5/30 | Loss: 0.1870 | Time: 21.5s\n",
      "Epoch  6/30 | Optimizing... | Time: 19.8s\n",
      "Epoch  7/30 | Optimizing... | Time: 18.1s\n",
      "Epoch  8/30 | Optimizing... | Time: 19.7s\n",
      "Epoch  9/30 | Optimizing... | Time: 22.1s\n",
      "Epoch 10/30 | Loss: 0.1756 | Time: 20.8s\n",
      "Epoch 11/30 | Optimizing... | Time: 22.7s\n",
      "Epoch 12/30 | Optimizing... | Time: 23.3s\n",
      "Epoch 13/30 | Optimizing... | Time: 22.6s\n",
      "Epoch 14/30 | Optimizing... | Time: 18.9s\n",
      "Epoch 15/30 | Loss: 0.1716 | Time: 19.5s\n",
      "Epoch 16/30 | Optimizing... | Time: 19.0s\n",
      "Epoch 17/30 | Optimizing... | Time: 20.6s\n",
      "Epoch 18/30 | Optimizing... | Time: 15.6s\n",
      "Epoch 19/30 | Optimizing... | Time: 18.0s\n",
      "Epoch 20/30 | Loss: 0.1783 | Time: 19.8s\n",
      "Epoch 21/30 | Optimizing... | Time: 18.1s\n",
      "Epoch 22/30 | Optimizing... | Time: 29.0s\n",
      "Epoch 23/30 | Optimizing... | Time: 18.5s\n",
      "Epoch 24/30 | Optimizing... | Time: 17.2s\n",
      "Epoch 25/30 | Loss: 0.1851 | Time: 19.6s\n",
      "Epoch 26/30 | Optimizing... | Time: 16.7s\n",
      "Epoch 27/30 | Optimizing... | Time: 21.0s\n",
      "Epoch 28/30 | Optimizing... | Time: 19.8s\n",
      "Epoch 29/30 | Optimizing... | Time: 16.5s\n",
      "Epoch 30/30 | Loss: 0.1664 | Time: 16.4s\n",
      "\n",
      " Quantum training complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 6: QUANTUM TRAINING\")\n",
    "\n",
    "np.random.seed(42)\n",
    "params = np.random.uniform(0, 2*np.pi, size=n_qubits * NUM_LAYERS)\n",
    "train_size = 2000\n",
    "fraud_idx = np.where(y_train == 1)[0]\n",
    "non_fraud_idx = np.where(y_train == 0)[0]\n",
    "\n",
    "n_fraud = min(train_size // 2, len(fraud_idx))\n",
    "n_non_fraud = train_size - n_fraud\n",
    "\n",
    "train_idx = np.concatenate([\n",
    "    np.random.choice(fraud_idx, n_fraud, replace=False),\n",
    "    np.random.choice(non_fraud_idx, n_non_fraud, replace=False)\n",
    "])\n",
    "\n",
    "X_train_qml = X_train_quantum[train_idx]\n",
    "y_train_qml = y_train.values[train_idx] if hasattr(y_train, 'values') else y_train[train_idx]\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Training samples: {len(X_train_qml)}\")\n",
    "print(f\"  Fraud: {y_train_qml.sum()} | Non-fraud: {len(y_train_qml) - y_train_qml.sum()}\")\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-9\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def compute_loss(params, X_subset, y_subset, shots=256):\n",
    "    \"\"\"Compute loss for optimizer\"\"\"\n",
    "    preds = []\n",
    "    for x in X_subset:\n",
    "        p = quantum_predict_proba(x, params, shots)\n",
    "        preds.append(p)\n",
    "    preds = np.array(preds)\n",
    "    return binary_cross_entropy(y_subset, preds)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "loss_history = []\n",
    "\n",
    "print(\"\\nStarting training with COBYLA optimizer...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = datetime.now()\n",
    "    \n",
    "    batch_idx = np.random.choice(len(X_train_qml), batch_size, replace=False)\n",
    "    X_batch = X_train_qml[batch_idx]\n",
    "    y_batch = y_train_qml[batch_idx]\n",
    "    \n",
    "    result = minimize(\n",
    "    compute_loss, \n",
    "    params, \n",
    "    args=(X_batch, y_batch, 512),\n",
    "    method='COBYLA',\n",
    "    options={\n",
    "        'maxiter': 60, \n",
    "        'rhobeg': 0.1  \n",
    "    }\n",
    "    )\n",
    "    params = result.x\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        train_loss = compute_loss(params, X_train_qml[:200], y_train_qml[:200], shots=512)\n",
    "        loss_history.append(train_loss)\n",
    "        \n",
    "        epoch_time = (datetime.now() - epoch_start).total_seconds()\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | Loss: {train_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
    "    else:\n",
    "        epoch_time = (datetime.now() - epoch_start).total_seconds()\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | Optimizing... | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "quantum_model_data = {\n",
    "    'params': params.tolist(),\n",
    "    'n_qubits': n_qubits,\n",
    "    'selected_features': selected_features,\n",
    "    'loss_history': loss_history\n",
    "}\n",
    "\n",
    "with open('output/models/quantum_model.json', 'w') as f:\n",
    "    json.dump(quantum_model_data, f, indent=2)\n",
    "\n",
    "print(\"\\n Quantum training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c6e86b-a4e3-497a-bad7-ffe1826e1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24ec80a-c498-4316-bb87-ee4bd0fbe487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: QUANTUM EVALUATION\n",
      "\n",
      "Evaluating on test set...\n",
      "  Processed 19500/19659 samples\n",
      "\n",
      "Quantum Model Performance:\n",
      "  Test AUC:       0.9994\n",
      "  Test Accuracy:  0.7962\n",
      "\n",
      " Quantum evaluation complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 7: QUANTUM EVALUATION\")\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "y_test_proba_quantum = []\n",
    "for i, x in enumerate(X_test_quantum):\n",
    "    if i % 500 == 0:\n",
    "        print(f\"  Processed {i}/{len(X_test_quantum)} samples\", end='\\r')\n",
    "    p = quantum_predict_proba(x, params, shots=2048)\n",
    "    y_test_proba_quantum.append(p)\n",
    "\n",
    "print() \n",
    "y_test_proba_quantum = np.array(y_test_proba_quantum)\n",
    "y_test_pred_quantum = (y_test_proba_quantum > 0.5).astype(int)\n",
    "\n",
    "results['Quantum VQC'] = {\n",
    "    'Test_AUC': roc_auc_score(y_test, y_test_proba_quantum),\n",
    "    'Test_Accuracy': accuracy_score(y_test, y_test_pred_quantum),\n",
    "    'Test_Precision': precision_score(y_test, y_test_pred_quantum),\n",
    "    'Test_Recall': recall_score(y_test, y_test_pred_quantum),\n",
    "    'Test_F1': f1_score(y_test, y_test_pred_quantum)\n",
    "}\n",
    "\n",
    "predictions['quantum'] = y_test_proba_quantum\n",
    "\n",
    "print(f\"\\nQuantum Model Performance:\")\n",
    "print(f\"  Test AUC:       {results['Quantum VQC']['Test_AUC']:.4f}\")\n",
    "print(f\"  Test Accuracy:  {results['Quantum VQC']['Test_Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n Quantum evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66053580-9e15-4abf-be05-2fe14a0cc5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
